"""
MNE Porting Verification Script
Generates test data for validating C++ implementation against Python MNE.

Usage:
    python gen_test_data.py
"""

import numpy as np
from scipy import signal
import os
import sys

# Add mne-python to path
mne_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../../mne-python"))
if os.path.exists(mne_path):
    sys.path.append(mne_path)
else:
    print(f"Warning: mne-python not found at {mne_path}")

try:
    from mne.time_frequency import tfr_array_morlet, morlet
except ImportError as e:
    print(f"Error importing MNE: {e}")
    # Fallback or exit?
    # For verification, we strictly need MNE to generate ground truth.
    # But maybe the user has 'mne' installed in the env separately?
    try:
        from mne.time_frequency import tfr_array_morlet, morlet
    except ImportError:
        print("Could not import mne. Please ensure mne-python is in the path or installed.")
        sys.exit(1)

OUTPUT_DIR = "data"

def ensure_dir(directory):
    if not os.path.exists(directory):
        os.makedirs(directory)

def generate_signal(n_samples=1000, sfreq=1000):
    """Generate a composite signal with known frequencies."""
    t = np.arange(n_samples) / sfreq
    # Signal: 10Hz + 20Hz + 50Hz noise
    sig = (np.sin(2 * np.pi * 10 * t) + 
           0.5 * np.sin(2 * np.pi * 20 * t) + 
           0.2 * np.random.randn(n_samples))
    return t, sig

def save_data(name, data):
    """Save numpy array to text file compatible with IOUtils::read_eigen_matrix."""
    filepath = os.path.join(OUTPUT_DIR, f"{name}.txt")
    # Save as space-separated values
    # Add header with dimensions
    header = f"Dimensions (rows x cols): {data.shape[0] if data.ndim > 1 else data.size} x {data.shape[1] if data.ndim > 1 else 1}\nDescription: Generated by gen_test_data.py"
    np.savetxt(filepath, data, header=header)
    print(f"Saved {name} to {filepath}")

def main():
    ensure_dir(OUTPUT_DIR)
    
    # 1. Basic Signal
    sfreq = 1000
    n_samples = 1000
    t, sig = generate_signal(n_samples, sfreq)
    
    save_data("signal_raw", sig)
    
    # 2. Hilbert Transform (Envelope)
    # Using scipy.signal.hilbert
    sig_analytic = signal.hilbert(sig)
    amplitude_envelope = np.abs(sig_analytic)
    save_data("signal_hilbert_abs", amplitude_envelope)
    
    # 3. Convolution (Simple Moving Average)
    # Window size 50 samples
    window = np.ones(50) / 50.0
    sig_conv = np.convolve(sig, window, mode='same')
    save_data("signal_conv_ma50", sig_conv)
    
    # 4. Window Functions
    n_win = 512
    # Hanning
    win_hanning = np.hanning(n_win)
    save_data("window_hanning_512", win_hanning)
    
    # Hamming
    win_hamming = np.hamming(n_win)
    save_data("window_hamming_512", win_hamming)
    
    # Blackman
    win_blackman = np.blackman(n_win)
    save_data("window_blackman_512", win_blackman)
    
    # 5. Phase 2: TFR
    print("Generating Phase 2 (TFR) data...")
    
    # 5.1 Morlet Wavelet
    freqs = np.array([10.0, 20.0])
    n_cycles = 5.0
    # mne.time_frequency.morlet returns list of arrays (one per freq)
    Ws = morlet(sfreq, freqs, n_cycles=n_cycles)
    
    for i, freq in enumerate(freqs):
        w = Ws[i]
        save_data(f"tfr_morlet_w_{int(freq)}hz_real", np.real(w))
        save_data(f"tfr_morlet_w_{int(freq)}hz_imag", np.imag(w))

    # 5.2 TFR Computation
    # Input needs to be (n_epochs, n_chans, n_times)
    data = sig[np.newaxis, np.newaxis, :] # 1 epoch, 1 channel
    
    # Compute TFR
    # output: (n_epochs, n_chans, n_freqs, n_times)
    power = tfr_array_morlet(data, sfreq, freqs, n_cycles=n_cycles, output='power')
    
    # Save power for each freq
    for i, freq in enumerate(freqs):
        p = power[0, 0, i, :]
        save_data(f"tfr_power_{int(freq)}hz", p)
    
    # 5.3 PSD Welch
    try:
        from mne.time_frequency import psd_array_welch
    except ImportError:
        # For older MNE or different structure
        try:
            from mne.time_frequency.psd import psd_array_welch
        except ImportError:
            print("Could not import psd_array_welch")
            psd_array_welch = None

    if psd_array_welch is not None:
        n_fft = 256
        # n_per_seg default is n_fft
        # output='power' default
        psds, freqs_psd = psd_array_welch(data, sfreq=sfreq, fmin=0, fmax=np.inf, 
                                          n_fft=n_fft, n_overlap=0, n_per_seg=n_fft, 
                                          average='mean', output='power')
        # psds: (n_epochs, n_chans, n_freqs)
        
        save_data("psd_welch_psds", psds[0, 0, :])
        save_data("psd_welch_freqs", freqs_psd)

    # 6. ICA
    print("Generating Phase 3 (ICA) data...")
    # Generate synthetic sources
    n_samples = 2000
    time = np.linspace(0, 8, n_samples)
    
    s1 = np.sin(2 * time)  # Sinusoid
    s2 = np.sign(np.sin(3 * time))  # Square wave
    s3 = signal.sawtooth(2 * np.pi * time)  # Sawtooth
    
    S = np.c_[s1, s2, s3]
    S += 0.2 * np.random.normal(size=S.shape)  # Add noise
    S /= S.std(axis=0)  # Standardize
    
    # Mixing matrix
    A = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]])  # 3 channels, 3 sources
    X = np.dot(S, A.T)  # (n_samples, n_channels)
    
    # MNE expects (n_channels, n_samples)
    X_mne = X.T
    
    save_data("ica_mixed_signal", X_mne)
    save_data("ica_true_sources", S.T)
    save_data("ica_mixing_matrix_true", A)
    
    # Run sklearn FastICA for reference
    from sklearn.decomposition import FastICA
    # sklearn FastICA takes (n_samples, n_features)
    transformer = FastICA(n_components=3, random_state=0, whiten='unit-variance')
    S_est = transformer.fit_transform(X)
    A_est = transformer.mixing_
    W_est = transformer.components_ # Unmixing * Whitening
    
    save_data("ica_sklearn_sources", S_est.T)
    save_data("ica_sklearn_mixing", A_est)
    save_data("ica_sklearn_components", W_est)

    # 7. Statistics
    print("Generating Phase 4 (Stats) data...")
    from scipy import stats
    
    # 7.1 T-Test
    # Group A: Mean 0, Std 1
    # Group B: Mean 0.5, Std 1
    np.random.seed(42)
    group_a = np.random.normal(0, 1, (20, 5)) # 20 samples, 5 features
    group_b = np.random.normal(0.5, 1, (20, 5))
    
    # 1-Sample T-test (Group B vs 0)
    t_1samp, p_1samp = stats.ttest_1samp(group_b, 0)
    save_data("stats_ttest_1samp_data", group_b)
    save_data("stats_ttest_1samp_t", t_1samp)
    save_data("stats_ttest_1samp_p", p_1samp)
    
    # Independent T-test (Group A vs Group B)
    t_ind, p_ind = stats.ttest_ind(group_a, group_b)
    save_data("stats_ttest_ind_data_a", group_a)
    save_data("stats_ttest_ind_data_b", group_b)
    save_data("stats_ttest_ind_t", t_ind)
    save_data("stats_ttest_ind_p", p_ind)
    
    # 7.2 Correction
    # Generate random p-values
    p_values = np.array([0.001, 0.01, 0.04, 0.05, 0.1, 0.5, 0.8])
    save_data("stats_p_values", p_values)
    
    # Bonferroni
    p_bonferroni = np.minimum(p_values * len(p_values), 1.0)
    save_data("stats_bonferroni", p_bonferroni)
    
    # FDR (Benjamini-Hochberg)
    from statsmodels.stats.multitest import multipletests
    try:
        # statsmodels might not be installed, use manual calc if fail
        reject, p_fdr, _, _ = multipletests(p_values, method='fdr_bh')
    except ImportError:
        print("statsmodels not found, skipping FDR generation (or manual calc)")
        # Manual FDR for reference
        # Sort p: 0.001, 0.01, 0.04, 0.05, 0.1, 0.5, 0.8
        # Ranks: 1, 2, 3, 4, 5, 6, 7. N=7.
        # q = p * N / rank
        # 0.001 * 7 / 1 = 0.007
        # 0.01 * 7 / 2 = 0.035
        # 0.04 * 7 / 3 = 0.0933
        # 0.05 * 7 / 4 = 0.0875 -> monotonic -> 0.0875
        # ...
        # For now let's rely on C++ implementation matching logical steps if we can't gen data easily.
        # But we should verify.
        p_fdr = np.zeros_like(p_values) # Placeholder
    
    if 'multipletests' in locals():
        save_data("stats_fdr", p_fdr)

    # 8. LCMV Beamformer
    print("Generating Phase 5 (LCMV) data...")
    # Simulate data
    # 5 channels, 2 sources, 1000 samples
    n_channels = 5
    n_sources = 2
    n_times = 1000
    
    # Leadfield (Random for simulation)
    np.random.seed(42)
    L = np.random.randn(n_channels, n_sources)
    save_data("lcmv_leadfield", L)
    
    # Sources (Sinusoids)
    t = np.linspace(0, 1, n_times)
    S = np.zeros((n_sources, n_times))
    S[0, :] = np.sin(2 * np.pi * 10 * t)
    S[1, :] = np.cos(2 * np.pi * 20 * t)
    
    # Noise
    noise = 0.1 * np.random.randn(n_channels, n_times)
    
    # Data
    X = np.dot(L, S) + noise
    save_data("lcmv_data", X)
    
    # Manual LCMV in Python to match C++ implementation
    # 1. Covariance
    # Center data
    X_mean = X.mean(axis=1, keepdims=True)
    X_centered = X - X_mean
    C = np.dot(X_centered, X_centered.T) / (n_times - 1)
    
    # 2. Regularize
    reg = 0.05
    trace = np.trace(C)
    avg_eig = trace / n_channels
    lambda_reg = reg * avg_eig
    C_reg = C + lambda_reg * np.eye(n_channels)
    C_inv = np.linalg.inv(C_reg)
    
    # 3. Weights
    # W = (L^T C^-1 L)^-1 L^T C^-1
    # num: (n_sources, n_channels)
    num = np.dot(L.T, C_inv)
    # den: (n_sources, n_sources)
    den = np.dot(num, L)
    den_inv = np.linalg.inv(den)
    W = np.dot(den_inv, num)
    
    save_data("lcmv_weights", W)
    
    # 4. Apply
    S_est = np.dot(W, X)
    save_data("lcmv_stc", S_est)

    print("Verification data generation complete.")

if __name__ == "__main__":
    main()
